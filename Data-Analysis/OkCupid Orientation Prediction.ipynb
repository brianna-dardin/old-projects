{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OkCupid Orientation Prediction\n",
    "\n",
    "Hi everyone! This notebook is a beginner's exploration of applying basic scikit-learn machine learning algorithms. I recently came across this OkCupid profile dataset (https://github.com/rudeboybert/JSE_OkCupid) and wondered if it was possible at all to predict a person's sexual orientation with all the other information available. Now in reality this is not a good question to focus on, since orientation is not likely to be easily predictable. But I decided on this question just for fun!\n",
    "\n",
    "Note that I already did a lot of analysis on the structure of the dataset beforehand, so what follows is the fruit of this previous labor.\n",
    "\n",
    "Before we do anything, we want to import Pandas. Pandas is absolutely essential for managing the data and preprocessing it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to load the OkCupid profile dataset into a Pandas dataframe so that we can actually do something with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "profile_data = pd.read_csv('http://briannadardin.com/profiles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the first step is to remove all the rows with missing values in the \"orientation\" column. We only want to train our models on data that have this attribute so that it can more reliably predict it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "profile_data = profile_data.dropna(subset=['orientation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to consider which columns to include in our analysis. Right off the bat, we want to drop all the essay columns, because all the values in these columns are unique to each individual. Now certainly we could build a predictive model off of generating all the words and/or n-grams and their frequencies, but we'll save that for a future project.\n",
    "\n",
    "With this particular dataset, I will also drop the location column, because the bulk of the users here are from the Bay Area. I might keep this column if we had a national sample of users, since some areas of the country do have a higher percentage of LGBT people. Since this is focused on one particular area, the precise differences in cities doesn't seem too relevant.\n",
    "\n",
    "I will also drop the \"speaks\" column, primarily because mapping not only every possible language but also their proficiency in each language would greatly increase dimensionality, with little likely predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "profile_data = profile_data.drop(['essay0','essay1','essay2','essay3','essay4',\n",
    "                                  'essay5','essay6','essay7','essay8','essay9',\n",
    "                                  'location','speaks'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at the \"last_online\" column. The dates by themselves don't seem to communicate a lot of information, but we can extract more useful information from these dates. For example, the day of the week could potentially be relevant, as well as the time of day (so we can capture early birds vs. night owls, for example). We could also calculate the most recent users (and therefore potentially more active) vs the users that signed on a comparatively long time ago. Let's tackle these one at a time.\n",
    "\n",
    "First with the day of the week, we create a simple function to extract the day of the week as an integer for each row, then add the column to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3\n",
       "1    4\n",
       "2    2\n",
       "3    3\n",
       "4    2\n",
       "Name: online_day, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "def day_of_week(lastOnline):\n",
    "    dt = datetime.datetime.strptime(lastOnline, '%Y-%m-%d-%H-%M')\n",
    "    return dt.weekday()\n",
    "\n",
    "profile_data['online_day'] = profile_data.apply(lambda row: day_of_week(row['last_online']), axis=1)\n",
    "\n",
    "profile_data['online_day'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we extract the time. In this case, I don't feel the actual minute is as relevant as just the hour, so we'll just extract the hour only and add that as a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    20\n",
       "1    21\n",
       "2     9\n",
       "3    14\n",
       "4    21\n",
       "Name: online_hour, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hour(lastOnline):\n",
    "    dt = datetime.datetime.strptime(lastOnline, '%Y-%m-%d-%H-%M')\n",
    "    return dt.hour\n",
    "\n",
    "profile_data['online_hour'] = profile_data.apply(lambda row: hour(row['last_online']), axis=1)\n",
    "\n",
    "profile_data['online_hour'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to see how recently a user has signed on, as this may indicate level of activity. First we have to get the most recent date from the column, and then compare all the other dates to this date and indicate how many days behind it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2\n",
       "1    1\n",
       "2    3\n",
       "3    2\n",
       "4    3\n",
       "Name: last_online_days, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_index = profile_data.sort_values(by='last_online', ascending=False).index[0]\n",
    "max_day = datetime.datetime.strptime(profile_data['last_online'][max_index], '%Y-%m-%d-%H-%M')\n",
    "\n",
    "def time_diff(lastOnline):\n",
    "    dt = datetime.datetime.strptime(lastOnline, '%Y-%m-%d-%H-%M')\n",
    "    time_delta = max_day - dt\n",
    "    return time_delta.days\n",
    "\n",
    "profile_data['last_online_days'] = profile_data.apply(lambda row: time_diff(row['last_online']), axis=1)\n",
    "\n",
    "profile_data['last_online_days'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've extracted the data we wanted from the last_online column, we can drop it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "profile_data = profile_data.drop('last_online',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the recommended approach for dealing with categorical variables is to \"one hot encode\" them. What this means is that, if a column has 5 distinct possible values, one hot encoding this column will result in creating 5 new columns, each corresponding to a particular value. The columns themselves are booleans - they will either be 1 if that value is true for the row, or 0 if not. So each row will then only have 1 column in this set of 5 with a value of 1, and the others will be 0s.\n",
    "\n",
    "However an issue arises when a column has a LOT of distinct possible values. The more values, the more columns, thus more work for our models. Some of the columns in this dataset have a lot of possible values - in fact, I purposely removed the \"speaks\" column earlier precisely for this reason. But what to do with all the other columns with multiple values?\n",
    "\n",
    "I ended up coming up with an unconventional approach - this may not be the \"best practice\" approach, but for our simple analysis, it will work. Basically what I noticed is that many of these columns are actually the combination of 2 drop down fields on the site itself. For example, the \"diet\" column is comprised of a drop down with \"strictly\" and \"mostly\" as options, and then another drop down with \"anything\", \"halal\", \"kosher\", \"other\", \"vegan\" and \"vegetarian\". The diet column then contains values such as \"strictly vegan\", \"mostly vegetarian\", etc. \n",
    "\n",
    "I decided that instead of one hot encoding all of these combined options, I will one hot encode the underlying drop down choices. What this means is that I will create a column corresponding to each value in the first drop down (\"strictly\" and \"mostly\") and to each value in the second drop down (\"anything\", \"halal\", etc). So each row will then either have 2 1s in this set of columns (1 corresponding to the first drop down, 1 corresponding to the second), just 1 (if they omitted the first drop down), or none (if they left both drop downs blank). \n",
    "\n",
    "This explanation may seem a little confusing, so let me show you in action. First, I created a nested dictionary for all the columns structured like this, and the unique values for their underlying drop downs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_columns = { 'diet': ['strictly', 'mostly', 'anything', 'halal', 'kosher', \n",
    "                         'other', 'vegan', 'vegetarian'],\n",
    "                'education': ['high school', 'college/university', 'law school', \n",
    "                              'masters program', 'med school', 'ph.d program', \n",
    "                              'space camp', 'two-year college', 'dropped out', \n",
    "                              'graduated', 'working on'],\n",
    "                'ethnicity': ['asian', 'black', 'hispanic / latin', 'indian', \n",
    "                             'middle eastern', 'native american', 'other', \n",
    "                             'pacific islander', 'white'],\n",
    "                'offspring': ['doesn&rsquo;t have kids', 'has a kid', 'has kids', \n",
    "                              'doesn&rsquo;t want', 'might want', 'wants'],\n",
    "                'religion': ['agnosticism', 'atheism', 'buddhism', 'catholicism', \n",
    "                             'christianity', 'hinduism', 'islam', 'judaism', \n",
    "                             'other', 'laughing', 'somewhat serious', \n",
    "                             'very serious', 'not too serious'],\n",
    "                'sign': ['aquarius', 'aries', 'cancer', 'capricorn', 'gemini', \n",
    "                         'leo', 'libra', 'pisces', 'sagittarius', 'scorpio', \n",
    "                         'taurus', 'virgo', 'matters a lot', \n",
    "                         'doesn&rsquo;t matter', 'fun to think about']\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this dictionary established, we can then iterate through this dictionary to generate our new columns. When iterating through the rows, we first look at each column in the dictionary. We then compare the value that row has in that column with the values in the dictionary. If there's a match between words/phrases, a 1 gets added to an array, otherwise a 0 is added. Each row's individual array gets added to the array of all the rows' arrays before moving on to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_column_data = []\n",
    "for index, row in profile_data.iterrows():\n",
    "    data_row = []\n",
    "    for key in new_columns.keys():\n",
    "        for val in new_columns[key]:\n",
    "            if pd.notnull(row[key]) and val in row[key]:\n",
    "                data_row.append(1)\n",
    "            else:\n",
    "                data_row.append(0)\n",
    "    new_column_data.append(data_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have that done, we want to then create an array with all the corresponding column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_column_names = []\n",
    "for key in new_columns.keys():\n",
    "    for val in new_columns[key]:\n",
    "        new_column_names.append(key+\"_\"+val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With both of those arrays compiled, we can combine them to create a new dataframe, and then merge that dataframe with the one we started with, and then drop the now redundant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_column_df = pd.DataFrame(data=new_column_data,columns=new_column_names)\n",
    "profile_data = pd.concat([profile_data,new_column_df],axis=1)\n",
    "profile_data = profile_data.drop(new_columns.keys(),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll try to clean up some missing values. For example, two columns in this dataset, \"body_type\" and \"job\" both have the \"rather not say\" option, yet there are still many missing values. A missing value is basically the same as \"rather not say\", so we'll convert them accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "profile_data['body_type'] = profile_data['body_type'].fillna('rather not say')\n",
    "profile_data['job'] = profile_data['job'].fillna('rather not say')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on, let's check for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                                      0\n",
      "body_type                                0\n",
      "drinks                                2985\n",
      "drugs                                14080\n",
      "height                                   3\n",
      "income                                   0\n",
      "job                                      0\n",
      "orientation                              0\n",
      "pets                                 19921\n",
      "sex                                      0\n",
      "smokes                                5512\n",
      "status                                   0\n",
      "online_day                               0\n",
      "online_hour                              0\n",
      "last_online_days                         0\n",
      "diet_strictly                            0\n",
      "diet_mostly                              0\n",
      "diet_anything                            0\n",
      "diet_halal                               0\n",
      "diet_kosher                              0\n",
      "diet_other                               0\n",
      "diet_vegan                               0\n",
      "diet_vegetarian                          0\n",
      "sign_aquarius                            0\n",
      "sign_aries                               0\n",
      "sign_cancer                              0\n",
      "sign_capricorn                           0\n",
      "sign_gemini                              0\n",
      "sign_leo                                 0\n",
      "sign_libra                               0\n",
      "                                     ...  \n",
      "religion_laughing                        0\n",
      "religion_somewhat serious                0\n",
      "religion_very serious                    0\n",
      "religion_not too serious                 0\n",
      "education_high school                    0\n",
      "education_college/university             0\n",
      "education_law school                     0\n",
      "education_masters program                0\n",
      "education_med school                     0\n",
      "education_ph.d program                   0\n",
      "education_space camp                     0\n",
      "education_two-year college               0\n",
      "education_dropped out                    0\n",
      "education_graduated                      0\n",
      "education_working on                     0\n",
      "offspring_doesn&rsquo;t have kids        0\n",
      "offspring_has a kid                      0\n",
      "offspring_has kids                       0\n",
      "offspring_doesn&rsquo;t want             0\n",
      "offspring_might want                     0\n",
      "offspring_wants                          0\n",
      "ethnicity_asian                          0\n",
      "ethnicity_black                          0\n",
      "ethnicity_hispanic / latin               0\n",
      "ethnicity_indian                         0\n",
      "ethnicity_middle eastern                 0\n",
      "ethnicity_native american                0\n",
      "ethnicity_other                          0\n",
      "ethnicity_pacific islander               0\n",
      "ethnicity_white                          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print profile_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few columns still have many null values, but that's because we haven't processed them yet. What stands out here however is that the \"height\" column only has 3 missing values. I think we can drop these rows without significantly impacting our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "profile_data = profile_data.dropna(subset=['height'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go ahead and properly one hot encode the columns I left out of the previous processing. This is also the stage where I decided to separate my target variable (\"orientation\") from the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = profile_data['orientation']\n",
    "features = profile_data.drop(['orientation'],axis=1)\n",
    "\n",
    "target_dummies = pd.get_dummies(data=target)\n",
    "feature_dummies = pd.get_dummies(data=features, dummy_na=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I used the parameter \"dummy_na=True\" for feature_dummies, because I wanted a column created for null values as well. However, this parameter creates a column for null values even if the original column doesn't contain any. So I want to remove those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nunique = feature_dummies.apply(pd.Series.nunique)\n",
    "cols_to_drop = nunique[nunique == 1].index\n",
    "feature_dummies = feature_dummies.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost ready to start testing various machine learning algorithms! Now we need to split our data into training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = target_dummies.values\n",
    "X = feature_dummies.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is just ONE more thing to do before we begin. See, these algorithms are fussy about what kind of targets they're willing to accept. They do not like multi-dimensional targets, like our current target, which has 3 possible values (in this dataset - out in the real world, of course, there are more): \"straight\", \"gay\", and \"bisexual\". Even though we one hot encoded this column properly, we still need to reduce the dimensionality of the target.\n",
    "\n",
    "To do this, we'll use NumPy's \"argmax\" function. Basically what this will do is record which index, aka which column, in the original had the highest value. Since only one of the columns will have a 1 in it, that column's index will be recorded. It preserves the meaning, just shrinks the target down from 3 columns to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_train = np.argmax(y_train, axis=1)\n",
    "y_test = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move on to testing various different algorithms! Note that I currently do not have an in depth understanding of any of the algorithms I'm about to use. This is basically just a survey of some common algorithms implemented in scikit-learn with their defaults left in tact, to see how well they perform on this particular problem. They each have their own optimal use cases, so none of the results here mean that a particular algorithm is \"good\" or \"bad\" - it just means whether it's well-suited to this particular problem or not. \n",
    "\n",
    "In order to evaluate each algorithm, we need to import 2 different metrics - the accuracy score and the confusion matrix. The accuracy score is just a percentage that tells you how often the algorithm was correct. This doesn't tell the whole story however. The confusion matrix shows us the breakdown of predictions - how many times it predicted a certain value, and how often it was correct in each value or not. In order to read the confusion matrix accurately, we want to be sure which order the columns were in. So we'll print those values for our reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bisexual', 'gay', 'straight']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print list(target_dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know to read our confusion matrices as representing \"bisexual\" in the first column, \"gay\" in the second column, and \"straight\" in the third column! This order also determines the row order, as we'll see later.\n",
    "\n",
    "Since I ran these previously, I decided on the order here based on a certain kind of logical flow. We'll look at each algorithm one at a time, and at the end decide which seems the most promising, in case we want to dive deeper later and get better results with some fine tuning.\n",
    "\n",
    "First up, Logistic Regression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy is  86.1035422343\n",
      "[[    0     0   836]\n",
      " [    0     0  1663]\n",
      " [    0     0 15484]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LogReg = LogisticRegression()\n",
    "LogReg.fit(X_train, y_train)\n",
    "y_pred_log = LogReg.predict(X_test)\n",
    "print \"Logistic Regression accuracy is \", accuracy_score(y_test,y_pred_log)*100\n",
    "log_matrix = confusion_matrix(y_test, y_pred_log)\n",
    "print log_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first you may be tempted to be like \"86% sounds pretty good!\", but then you look at the confusion matrix. It literally scored that high by assuming that everyone is straight (as all the values are in the third column). Who knew that logistic regression is homophobic? This is definitely not the approach we would want in solving this problem, so maybe we'll get better results with something else? Let's find out!\n",
    "\n",
    "Next up, Naive Bayes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy is  85.3583940388\n",
      "[[   12     0   824]\n",
      " [   11     0  1652]\n",
      " [  146     0 15338]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred_gnb = gnb.predict(X_test)\n",
    "print \"Naive Bayes accuracy is \", accuracy_score(y_test,y_pred_gnb)*100\n",
    "gnb_matrix = confusion_matrix(y_test, y_pred_gnb)\n",
    "print gnb_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score is slightly lower, but we're getting a little better when it comes to confusion matrix. This one acknowledges that bisexual people exist (first column), but completely ignores gay people (second column). So it's still not what we're looking for. \n",
    "\n",
    "Let's try a different one then. Next up, K-Nearest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-nearest accuracy is  84.6521714953\n",
      "[[   12    15   809]\n",
      " [   12    50  1601]\n",
      " [   75   248 15161]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knc = KNeighborsClassifier()\n",
    "knc.fit(X_train, y_train)\n",
    "y_pred_knc = knc.predict(X_test)\n",
    "print \"K-nearest accuracy is \", accuracy_score(y_test,y_pred_knc)*100\n",
    "knc_matrix = confusion_matrix(y_test, y_pred_knc)\n",
    "print knc_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're getting somewhere! Sure, the accuracy score is a little lower, but it actually acknowledges both bisexual and gay pepole! Finally making some progress! Note that the rows are in the same order as the columns, so first row = \"bisexual\", second row = \"gay\", third row = \"straight\". Here is a step-by-step way of reading the matrix (which will apply for all the future ones in this notebook):\n",
    "\n",
    "first column, first row = bisexual prediction, actually bisexual  \n",
    "first column, second row = bisexual prediction, actually gay  \n",
    "first column, third row = bisexual prediction, actually straight  \n",
    "second column, first row = gay prediction, actually bisexual  \n",
    "second column, second row = gay prediction, actually gay  \n",
    "second colum, third row = gay prediction, actually straight  \n",
    "third column, first row = straight prediction, actually bisexual  \n",
    "third column, second row = straight prediction, actually gay  \n",
    "third column, third row = straight prediction, actually straight  \n",
    "\n",
    "Those aren't particularly impressive scores, can we do better than that? Let's find out! Next up is a Decision Tree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree accuracy is  76.8948451315\n",
      "[[  147    98   591]\n",
      " [   79   276  1308]\n",
      " [  625  1454 13405]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dec_tree = DecisionTreeClassifier()\n",
    "dec_tree.fit(X_train, y_train)\n",
    "y_pred_tree = dec_tree.predict(X_test)\n",
    "print \"Decision Tree accuracy is \", accuracy_score(y_test,y_pred_tree)*100\n",
    "tree_matrix = confusion_matrix(y_test, y_pred_tree)\n",
    "print tree_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is easily the lowest accuracy score we've gotten so far, but look at that matrix! Finally, an algorithm that cares about bisexual and gay people! \n",
    "\n",
    "Since it seems like we're on the right track, we'll try one more algorithm - Random Forests, which is basically Decision Trees on steroids. Let's see what we get!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest accuracy is  85.775454596\n",
      "[[   61     8   767]\n",
      " [   21    52  1590]\n",
      " [   71   101 15312]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier()\n",
    "forest.fit(X_train, y_train)\n",
    "y_pred_forest = forest.predict(X_test)\n",
    "print \"Random Forest accuracy is \", accuracy_score(y_test,y_pred_forest)*100\n",
    "forest_matrix = confusion_matrix(y_test, y_pred_forest)\n",
    "print forest_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a higher accuracy percentage, but also lower numbers of predictions for bisexual and gay people.\n",
    "\n",
    "Out of all the algorithms we sampled here, Random Forest seems to have the best ratio of accuracy score and gay/bisexual predictions, so this may be the best one to pursue further. However, Decision Trees has the highest number of correct gay/bisexual predictions, so that may also be the best one to pursue. There are also many other algorithms I left out of this notebook - maybe one of those could work better with some fine tuning.\n",
    "\n",
    "Ultimately though, looking at all of these numbers, it is easy to see that accurately predicting someone's orientation based off their OkCupid data is ultimately not going to be very accurate. Thankfully we already realized this at the outset - this was simply to get our toes wet in preparing data for use by machine learning algorithms. :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
